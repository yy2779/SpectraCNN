{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.set_printoptions(precision=1)\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from modules.utils import load_cifar10\n",
    "# from modules.cnn_with_spectral_parameterization import CNN_Spectral_Param\n",
    "# from modules.cnn_with_spectral_pooling import CNN_Spectral_Pool\n",
    "from modules.image_generator import ImageGenerator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import pytorch_fft.fft.autograd as fft\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already downloaded..\n",
      "getting batch 1\n"
     ]
    }
   ],
   "source": [
    "# In the interest of training time, we only used 1 of 5 cifar10 batches\n",
    "# The important part of the experiment is to compare the rates of convergence of training accuracy,\n",
    "# so subsetting the training dataset for both spectral and spatial models shouldn't impact\n",
    "# the relationship between their train accuracy convergences\n",
    "xtrain, ytrain, xtest, ytest = load_cifar10(1, channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 3, 32, 32), (10000,), (10000, 3, 32, 32), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralParam(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(SpectralParam, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.ifft = fft.Ifft2d()\n",
    "        \n",
    "        weight = torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).cuda()\n",
    "        nn.init.xavier_uniform(weight)\n",
    "        weight_re, weight_im = fft.fft2(weight, torch.zeros_like(weight).cuda())\n",
    "        \n",
    "        self.weight_re = nn.Parameter(weight_re, requires_grad=True)\n",
    "        self.weight_im = nn.Parameter(weight_im, requires_grad=True)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels), requires_grad=True)\n",
    "            nn.init.normal(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        weight, _ = self.ifft(self.weight_re, self.weight_im)\n",
    "#         weight, _ = self.ifft(self.weight_re, torch.zeros_like(self.weight_re).cuda())\n",
    "        result = F.conv2d(input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  -2.4363  -2.9699  -3.5035\n",
      "  -4.5707  -5.1043  -5.6379\n",
      "  -6.7050  -7.2386  -7.7722\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  -1.9191  -2.8061  -3.6930\n",
      "  -5.4670  -6.3539  -7.2409\n",
      "  -9.0149  -9.9018 -10.7888\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  -0.5217   0.0315   0.5847\n",
      "   1.6911   2.2443   2.7975\n",
      "   3.9039   4.4571   5.0103\n",
      "[torch.cuda.FloatTensor of size 1x3x3x3 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "   1.5356   1.8990   3.2706  -0.8773\n",
      "   9.3115  13.5115  16.2664   1.9149\n",
      "  19.7047  24.5312  27.2862   2.5415\n",
      "  12.6273  17.1937  18.5771   3.5221\n",
      "[torch.cuda.FloatTensor of size 1x1x4x4 (GPU 0)]\n",
      " Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -7.9793e+02  4.5938e+01\n",
      "  1.8375e+02 -1.5259e-05\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -1.0387e+03  5.7186e+01\n",
      "  2.2874e+02  1.5259e-05\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  4.1583e+02 -2.0198e+01\n",
      " -8.0794e+01  7.6294e-06\n",
      "[torch.cuda.FloatTensor of size 3x1x2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = Variable(torch.arange(16).view(1, 1, 4, 4).cuda(), requires_grad=True)\n",
    "model = SpectralParam(1, 3, 2).cuda()\n",
    "\n",
    "B = model(A)\n",
    "print(B)\n",
    "C = torch.sum(B * B)\n",
    "C.backward()\n",
    "print(A.grad, model.weight_re.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SpectralParam(3, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = SpectralParam(96, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 8 * 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  train loss: 2.486  train acc: 0.096  val acc: 0.097\n",
      "epoch: 2  train loss: 2.307  train acc: 0.107  val acc: 0.104\n",
      "epoch: 3  train loss: 2.255  train acc: 0.151  val acc: 0.143\n",
      "epoch: 4  train loss: 2.171  train acc: 0.178  val acc: 0.192\n",
      "epoch: 5  train loss: 2.065  train acc: 0.221  val acc: 0.216\n",
      "epoch: 6  train loss: 1.929  train acc: 0.271  val acc: 0.283\n",
      "epoch: 7  train loss: 1.805  train acc: 0.318  val acc: 0.328\n",
      "epoch: 8  train loss: 1.719  train acc: 0.355  val acc: 0.327\n",
      "epoch: 9  train loss: 1.672  train acc: 0.376  val acc: 0.375\n",
      "epoch: 10  train loss: 1.601  train acc: 0.409  val acc: 0.408\n",
      "epoch: 11  train loss: 1.567  train acc: 0.424  val acc: 0.422\n",
      "epoch: 12  train loss: 1.506  train acc: 0.444  val acc: 0.432\n",
      "epoch: 13  train loss: 1.452  train acc: 0.465  val acc: 0.421\n",
      "epoch: 14  train loss: 1.416  train acc: 0.484  val acc: 0.475\n",
      "epoch: 15  train loss: 1.362  train acc: 0.504  val acc: 0.487\n",
      "epoch: 16  train loss: 1.311  train acc: 0.519  val acc: 0.487\n",
      "epoch: 17  train loss: 1.296  train acc: 0.530  val acc: 0.503\n",
      "epoch: 18  train loss: 1.227  train acc: 0.556  val acc: 0.505\n",
      "epoch: 19  train loss: 1.225  train acc: 0.553  val acc: 0.486\n",
      "epoch: 20  train loss: 1.152  train acc: 0.584  val acc: 0.508\n",
      "epoch: 21  train loss: 1.104  train acc: 0.603  val acc: 0.505\n",
      "epoch: 22  train loss: 1.077  train acc: 0.610  val acc: 0.529\n",
      "epoch: 23  train loss: 1.040  train acc: 0.624  val acc: 0.545\n",
      "epoch: 24  train loss: 0.997  train acc: 0.644  val acc: 0.551\n",
      "epoch: 25  train loss: 0.958  train acc: 0.654  val acc: 0.556\n",
      "epoch: 26  train loss: 0.927  train acc: 0.668  val acc: 0.539\n",
      "epoch: 27  train loss: 0.920  train acc: 0.664  val acc: 0.544\n",
      "epoch: 28  train loss: 0.858  train acc: 0.696  val acc: 0.551\n",
      "epoch: 29  train loss: 0.824  train acc: 0.700  val acc: 0.576\n",
      "epoch: 30  train loss: 0.768  train acc: 0.729  val acc: 0.561\n",
      "epoch: 31  train loss: 0.748  train acc: 0.738  val acc: 0.570\n",
      "epoch: 32  train loss: 0.711  train acc: 0.749  val acc: 0.574\n",
      "epoch: 33  train loss: 0.691  train acc: 0.753  val acc: 0.568\n",
      "epoch: 34  train loss: 0.665  train acc: 0.767  val acc: 0.576\n",
      "epoch: 35  train loss: 0.606  train acc: 0.793  val acc: 0.587\n",
      "epoch: 36  train loss: 0.559  train acc: 0.811  val acc: 0.573\n",
      "epoch: 37  train loss: 0.526  train acc: 0.826  val acc: 0.579\n",
      "epoch: 38  train loss: 0.494  train acc: 0.830  val acc: 0.569\n",
      "epoch: 39  train loss: 0.449  train acc: 0.855  val acc: 0.588\n",
      "epoch: 40  train loss: 0.422  train acc: 0.856  val acc: 0.589\n",
      "epoch: 41  train loss: 0.377  train acc: 0.882  val acc: 0.581\n",
      "epoch: 42  train loss: 0.349  train acc: 0.887  val acc: 0.573\n",
      "epoch: 43  train loss: 0.335  train acc: 0.894  val acc: 0.574\n",
      "epoch: 44  train loss: 0.314  train acc: 0.903  val acc: 0.584\n",
      "epoch: 45  train loss: 0.327  train acc: 0.897  val acc: 0.569\n",
      "epoch: 46  train loss: 0.274  train acc: 0.918  val acc: 0.580\n",
      "epoch: 47  train loss: 0.242  train acc: 0.931  val acc: 0.574\n",
      "epoch: 48  train loss: 0.213  train acc: 0.943  val acc: 0.587\n",
      "epoch: 49  train loss: 0.199  train acc: 0.952  val acc: 0.587\n",
      "epoch: 50  train loss: 0.182  train acc: 0.957  val acc: 0.576\n",
      "epoch: 51  train loss: 0.177  train acc: 0.955  val acc: 0.583\n",
      "epoch: 52  train loss: 0.151  train acc: 0.965  val acc: 0.583\n",
      "epoch: 53  train loss: 0.140  train acc: 0.970  val acc: 0.579\n",
      "epoch: 54  train loss: 0.124  train acc: 0.977  val acc: 0.585\n",
      "epoch: 55  train loss: 0.113  train acc: 0.978  val acc: 0.571\n",
      "epoch: 56  train loss: 0.125  train acc: 0.974  val acc: 0.572\n",
      "epoch: 57  train loss: 0.118  train acc: 0.975  val acc: 0.574\n",
      "epoch: 58  train loss: 0.101  train acc: 0.983  val acc: 0.575\n",
      "epoch: 59  train loss: 0.081  train acc: 0.989  val acc: 0.581\n",
      "epoch: 60  train loss: 0.102  train acc: 0.980  val acc: 0.583\n",
      "epoch: 61  train loss: 0.073  train acc: 0.992  val acc: 0.580\n",
      "epoch: 62  train loss: 0.063  train acc: 0.992  val acc: 0.583\n",
      "epoch: 63  train loss: 0.054  train acc: 0.996  val acc: 0.586\n",
      "epoch: 64  train loss: 0.051  train acc: 0.996  val acc: 0.580\n",
      "epoch: 65  train loss: 0.050  train acc: 0.995  val acc: 0.582\n",
      "epoch: 66  train loss: 0.053  train acc: 0.995  val acc: 0.584\n",
      "epoch: 67  train loss: 0.048  train acc: 0.996  val acc: 0.583\n",
      "epoch: 68  train loss: 0.046  train acc: 0.996  val acc: 0.580\n",
      "epoch: 69  train loss: 0.042  train acc: 0.997  val acc: 0.580\n",
      "epoch: 70  train loss: 0.043  train acc: 0.996  val acc: 0.580\n",
      "epoch: 71  train loss: 0.042  train acc: 0.997  val acc: 0.579\n",
      "epoch: 72  train loss: 0.041  train acc: 0.997  val acc: 0.582\n",
      "epoch: 73  train loss: 0.041  train acc: 0.997  val acc: 0.580\n",
      "epoch: 74  train loss: 0.041  train acc: 0.997  val acc: 0.579\n",
      "epoch: 75  train loss: 0.043  train acc: 0.997  val acc: 0.573\n",
      "epoch: 76  train loss: 0.041  train acc: 0.997  val acc: 0.580\n",
      "epoch: 77  train loss: 0.047  train acc: 0.996  val acc: 0.566\n",
      "epoch: 78  train loss: 0.066  train acc: 0.990  val acc: 0.573\n",
      "epoch: 79  train loss: 0.116  train acc: 0.967  val acc: 0.557\n",
      "epoch: 80  train loss: 0.123  train acc: 0.964  val acc: 0.572\n",
      "epoch: 81  train loss: 0.060  train acc: 0.989  val acc: 0.579\n",
      "epoch: 82  train loss: 0.030  train acc: 0.997  val acc: 0.581\n",
      "epoch: 83  train loss: 0.025  train acc: 0.997  val acc: 0.584\n",
      "epoch: 84  train loss: 0.025  train acc: 0.997  val acc: 0.580\n",
      "epoch: 85  train loss: 0.026  train acc: 0.997  val acc: 0.581\n",
      "epoch: 86  train loss: 0.026  train acc: 0.997  val acc: 0.579\n",
      "epoch: 87  train loss: 0.025  train acc: 0.997  val acc: 0.578\n",
      "epoch: 88  train loss: 0.027  train acc: 0.997  val acc: 0.582\n",
      "epoch: 89  train loss: 0.027  train acc: 0.997  val acc: 0.577\n",
      "epoch: 90  train loss: 0.029  train acc: 0.997  val acc: 0.582\n",
      "epoch: 91  train loss: 0.027  train acc: 0.997  val acc: 0.575\n",
      "epoch: 92  train loss: 0.030  train acc: 0.997  val acc: 0.572\n",
      "epoch: 93  train loss: 0.030  train acc: 0.997  val acc: 0.578\n",
      "epoch: 94  train loss: 0.030  train acc: 0.997  val acc: 0.582\n",
      "epoch: 95  train loss: 0.027  train acc: 0.997  val acc: 0.577\n",
      "epoch: 96  train loss: 0.028  train acc: 0.997  val acc: 0.581\n",
      "epoch: 97  train loss: 0.034  train acc: 0.996  val acc: 0.577\n",
      "epoch: 98  train loss: 0.033  train acc: 0.997  val acc: 0.583\n",
      "epoch: 99  train loss: 0.040  train acc: 0.996  val acc: 0.571\n",
      "epoch: 100  train loss: 0.062  train acc: 0.990  val acc: 0.576\n",
      "test acc: 0.574\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain[:-4096], ytrain[:-4096])\n",
    "    val_gen = ImageGenerator(xtrain[-4096:], ytrain[-4096:])\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    val_generator = val_gen.next_batch_gen(batch_size)\n",
    "    \n",
    "    iters = int((xtrain.shape[0] - 4096) / batch_size)\n",
    "    val_iters = int(4096 / batch_size)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        start = time.time()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        train_loss = np.mean(loss_iter)\n",
    "        train_acc = np.sum(acc_iter) / (xtrain.shape[0] - 4096)\n",
    "        \n",
    "        # validation\n",
    "        val_iter = []\n",
    "        for itr in range(val_iters):\n",
    "            X_batch, y_batch = next(val_generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "            val_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        val_acc = np.sum(val_iter) / 4096\n",
    "        \n",
    "        print('epoch: %d  train loss: %.3f  train acc: %.3f  val acc: %.3f' % (epoch + 1, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # test the network\n",
    "    test_gen = ImageGenerator(xtest, ytest)\n",
    "    generator = test_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtest.shape[0] / batch_size)\n",
    "    test_iter = []\n",
    "    for itr in range(iters):\n",
    "        X_batch, y_batch = next(val_generator)\n",
    "        inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "        labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "        outputs = net.forward(inputs)\n",
    "            \n",
    "        _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "        test_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "    test_acc = np.sum(test_iter) / xtest.shape[0]\n",
    "        \n",
    "    print('test acc: %.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SpectralParam(3, 128, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = SpectralParam(128, 160, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv3 = SpectralParam(160, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv4 = SpectralParam(192, 192, kernel_size=1, padding=0)\n",
    "        self.conv5 = SpectralParam(192, 10, kernel_size=1, padding=0)\n",
    "        \n",
    "        self.avg = nn.AvgPool2d(4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.conv5(F.relu(self.conv4(x)))\n",
    "        \n",
    "        return torch.squeeze(self.avg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  train loss: 2.406  train acc: 0.098  val acc: 0.103\n",
      "epoch: 2  train loss: 2.296  train acc: 0.121  val acc: 0.165\n",
      "epoch: 3  train loss: 2.147  train acc: 0.186  val acc: 0.182\n",
      "epoch: 4  train loss: 2.020  train acc: 0.226  val acc: 0.231\n",
      "epoch: 5  train loss: 1.914  train acc: 0.275  val acc: 0.306\n",
      "epoch: 6  train loss: 1.828  train acc: 0.314  val acc: 0.334\n",
      "epoch: 7  train loss: 1.759  train acc: 0.338  val acc: 0.340\n",
      "epoch: 8  train loss: 1.730  train acc: 0.359  val acc: 0.375\n",
      "epoch: 9  train loss: 1.647  train acc: 0.386  val acc: 0.381\n",
      "epoch: 10  train loss: 1.632  train acc: 0.401  val acc: 0.395\n",
      "epoch: 11  train loss: 1.588  train acc: 0.409  val acc: 0.374\n",
      "epoch: 12  train loss: 1.573  train acc: 0.423  val acc: 0.408\n",
      "epoch: 13  train loss: 1.517  train acc: 0.440  val acc: 0.440\n",
      "epoch: 14  train loss: 1.512  train acc: 0.451  val acc: 0.412\n",
      "epoch: 15  train loss: 1.481  train acc: 0.457  val acc: 0.423\n",
      "epoch: 16  train loss: 1.435  train acc: 0.472  val acc: 0.465\n",
      "epoch: 17  train loss: 1.407  train acc: 0.490  val acc: 0.442\n",
      "epoch: 18  train loss: 1.393  train acc: 0.489  val acc: 0.483\n",
      "epoch: 19  train loss: 1.366  train acc: 0.503  val acc: 0.480\n",
      "epoch: 20  train loss: 1.340  train acc: 0.518  val acc: 0.487\n",
      "epoch: 21  train loss: 1.333  train acc: 0.514  val acc: 0.459\n",
      "epoch: 22  train loss: 1.311  train acc: 0.524  val acc: 0.484\n",
      "epoch: 23  train loss: 1.297  train acc: 0.533  val acc: 0.486\n",
      "epoch: 24  train loss: 1.271  train acc: 0.539  val acc: 0.469\n",
      "epoch: 25  train loss: 1.283  train acc: 0.529  val acc: 0.449\n",
      "epoch: 26  train loss: 1.250  train acc: 0.550  val acc: 0.499\n",
      "epoch: 27  train loss: 1.224  train acc: 0.558  val acc: 0.512\n",
      "epoch: 28  train loss: 1.208  train acc: 0.560  val acc: 0.524\n",
      "epoch: 29  train loss: 1.213  train acc: 0.558  val acc: 0.525\n",
      "epoch: 30  train loss: 1.184  train acc: 0.575  val acc: 0.527\n",
      "epoch: 31  train loss: 1.151  train acc: 0.586  val acc: 0.535\n",
      "epoch: 32  train loss: 1.131  train acc: 0.593  val acc: 0.539\n",
      "epoch: 33  train loss: 1.130  train acc: 0.594  val acc: 0.529\n",
      "epoch: 34  train loss: 1.101  train acc: 0.604  val acc: 0.544\n",
      "epoch: 35  train loss: 1.080  train acc: 0.614  val acc: 0.519\n",
      "epoch: 36  train loss: 1.091  train acc: 0.600  val acc: 0.538\n",
      "epoch: 37  train loss: 1.066  train acc: 0.614  val acc: 0.552\n",
      "epoch: 38  train loss: 1.044  train acc: 0.626  val acc: 0.559\n",
      "epoch: 39  train loss: 1.015  train acc: 0.637  val acc: 0.548\n",
      "epoch: 40  train loss: 1.015  train acc: 0.638  val acc: 0.571\n",
      "epoch: 41  train loss: 0.965  train acc: 0.654  val acc: 0.541\n",
      "epoch: 42  train loss: 0.971  train acc: 0.643  val acc: 0.566\n",
      "epoch: 43  train loss: 0.946  train acc: 0.663  val acc: 0.548\n",
      "epoch: 44  train loss: 0.936  train acc: 0.665  val acc: 0.574\n",
      "epoch: 45  train loss: 0.951  train acc: 0.657  val acc: 0.548\n",
      "epoch: 46  train loss: 0.899  train acc: 0.679  val acc: 0.581\n",
      "epoch: 47  train loss: 0.877  train acc: 0.686  val acc: 0.579\n",
      "epoch: 48  train loss: 0.879  train acc: 0.684  val acc: 0.542\n",
      "epoch: 49  train loss: 0.870  train acc: 0.688  val acc: 0.590\n",
      "epoch: 50  train loss: 0.865  train acc: 0.690  val acc: 0.580\n",
      "epoch: 51  train loss: 0.804  train acc: 0.714  val acc: 0.583\n",
      "epoch: 52  train loss: 0.793  train acc: 0.719  val acc: 0.584\n",
      "epoch: 53  train loss: 0.829  train acc: 0.698  val acc: 0.589\n",
      "epoch: 54  train loss: 0.781  train acc: 0.721  val acc: 0.575\n",
      "epoch: 55  train loss: 0.769  train acc: 0.730  val acc: 0.580\n",
      "epoch: 56  train loss: 0.769  train acc: 0.728  val acc: 0.589\n",
      "epoch: 57  train loss: 0.773  train acc: 0.717  val acc: 0.572\n",
      "epoch: 58  train loss: 0.717  train acc: 0.751  val acc: 0.569\n",
      "epoch: 59  train loss: 0.721  train acc: 0.747  val acc: 0.593\n",
      "epoch: 60  train loss: 0.688  train acc: 0.764  val acc: 0.585\n",
      "epoch: 61  train loss: 0.661  train acc: 0.769  val acc: 0.583\n",
      "epoch: 62  train loss: 0.645  train acc: 0.779  val acc: 0.589\n",
      "epoch: 63  train loss: 0.644  train acc: 0.781  val acc: 0.579\n",
      "epoch: 64  train loss: 0.639  train acc: 0.780  val acc: 0.591\n",
      "epoch: 65  train loss: 0.635  train acc: 0.783  val acc: 0.587\n",
      "epoch: 66  train loss: 0.604  train acc: 0.798  val acc: 0.600\n",
      "epoch: 67  train loss: 0.590  train acc: 0.800  val acc: 0.593\n",
      "epoch: 68  train loss: 0.596  train acc: 0.799  val acc: 0.572\n",
      "epoch: 69  train loss: 0.577  train acc: 0.803  val acc: 0.567\n",
      "epoch: 70  train loss: 0.574  train acc: 0.804  val acc: 0.577\n",
      "epoch: 71  train loss: 0.547  train acc: 0.817  val acc: 0.602\n",
      "epoch: 72  train loss: 0.532  train acc: 0.827  val acc: 0.595\n",
      "epoch: 73  train loss: 0.551  train acc: 0.812  val acc: 0.599\n",
      "epoch: 74  train loss: 0.541  train acc: 0.816  val acc: 0.601\n",
      "epoch: 75  train loss: 0.521  train acc: 0.831  val acc: 0.573\n",
      "epoch: 76  train loss: 0.513  train acc: 0.836  val acc: 0.594\n",
      "epoch: 77  train loss: 0.504  train acc: 0.832  val acc: 0.574\n",
      "epoch: 78  train loss: 0.502  train acc: 0.834  val acc: 0.601\n",
      "epoch: 79  train loss: 0.491  train acc: 0.837  val acc: 0.583\n",
      "epoch: 80  train loss: 0.479  train acc: 0.842  val acc: 0.594\n",
      "epoch: 81  train loss: 0.488  train acc: 0.838  val acc: 0.579\n",
      "epoch: 82  train loss: 0.442  train acc: 0.864  val acc: 0.603\n",
      "epoch: 83  train loss: 0.430  train acc: 0.867  val acc: 0.599\n",
      "epoch: 84  train loss: 0.402  train acc: 0.886  val acc: 0.595\n",
      "epoch: 85  train loss: 0.429  train acc: 0.861  val acc: 0.579\n",
      "epoch: 86  train loss: 0.417  train acc: 0.872  val acc: 0.585\n",
      "epoch: 87  train loss: 0.395  train acc: 0.885  val acc: 0.603\n",
      "epoch: 88  train loss: 0.394  train acc: 0.877  val acc: 0.582\n",
      "epoch: 89  train loss: 0.381  train acc: 0.890  val acc: 0.589\n",
      "epoch: 90  train loss: 0.381  train acc: 0.889  val acc: 0.594\n",
      "epoch: 91  train loss: 0.386  train acc: 0.885  val acc: 0.592\n",
      "epoch: 92  train loss: 0.413  train acc: 0.863  val acc: 0.571\n",
      "epoch: 93  train loss: 0.381  train acc: 0.887  val acc: 0.592\n",
      "epoch: 94  train loss: 0.347  train acc: 0.904  val acc: 0.594\n",
      "epoch: 95  train loss: 0.358  train acc: 0.893  val acc: 0.590\n",
      "epoch: 96  train loss: 0.338  train acc: 0.908  val acc: 0.595\n",
      "epoch: 97  train loss: 0.354  train acc: 0.897  val acc: 0.590\n",
      "epoch: 98  train loss: 0.331  train acc: 0.908  val acc: 0.591\n",
      "epoch: 99  train loss: 0.320  train acc: 0.913  val acc: 0.597\n",
      "epoch: 100  train loss: 0.319  train acc: 0.910  val acc: 0.594\n",
      "test acc: 0.593\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 128\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 1e-3\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain[:-4096], ytrain[:-4096])\n",
    "    val_gen = ImageGenerator(xtrain[-4096:], ytrain[-4096:])\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    val_generator = val_gen.next_batch_gen(batch_size)\n",
    "    \n",
    "    iters = int((xtrain.shape[0] - 4096) / batch_size)\n",
    "    val_iters = int(4096 / batch_size)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        start = time.time()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        train_loss = np.mean(loss_iter)\n",
    "        train_acc = np.sum(acc_iter) / (xtrain.shape[0] - 4096)\n",
    "        \n",
    "        # validation\n",
    "        val_iter = []\n",
    "        for itr in range(val_iters):\n",
    "            X_batch, y_batch = next(val_generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "            val_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        val_acc = np.sum(val_iter) / 4096\n",
    "        \n",
    "        print('epoch: %d  train loss: %.3f  train acc: %.3f  val acc: %.3f' % (epoch + 1, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # test the network\n",
    "    test_gen = ImageGenerator(xtest, ytest)\n",
    "    generator = test_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtest.shape[0] / batch_size)\n",
    "    test_iter = []\n",
    "    for itr in range(iters):\n",
    "        X_batch, y_batch = next(val_generator)\n",
    "        inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "        labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "        outputs = net.forward(inputs)\n",
    "            \n",
    "        _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "        test_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "    test_acc = np.sum(test_iter) / xtest.shape[0]\n",
    "        \n",
    "    print('test acc: %.3f' % (test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
