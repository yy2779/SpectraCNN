{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.set_printoptions(precision=1)\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from modules.utils import load_cifar10\n",
    "# from modules.cnn_with_spectral_parameterization import CNN_Spectral_Param\n",
    "# from modules.cnn_with_spectral_pooling import CNN_Spectral_Pool\n",
    "from modules.image_generator import ImageGenerator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import pytorch_fft.fft.autograd as fft\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already downloaded..\n",
      "getting batch 1\n",
      "getting batch 2\n",
      "getting batch 3\n",
      "getting batch 4\n",
      "getting batch 5\n"
     ]
    }
   ],
   "source": [
    "# In the interest of training time, we only used 1 of 5 cifar10 batches\n",
    "# The important part of the experiment is to compare the rates of convergence of training accuracy,\n",
    "# so subsetting the training dataset for both spectral and spatial models shouldn't impact\n",
    "# the relationship between their train accuracy convergences\n",
    "xtrain, ytrain, xtest, ytest = load_cifar10(5, channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (50000,), (10000, 3, 32, 32), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_spectral_pool(images, filter_size):\n",
    "    assert (torch.ge(filter_size, 3)).all()\n",
    "    assert images.size()[-1] == images.size()[-2] and images.size()[-1] >= 3\n",
    "    \n",
    "    if int(filter_size) % 2 == 1:\n",
    "        n = int((filter_size - 1)/2)\n",
    "        top_left = images[:, :, :n+1, :n+1]\n",
    "        top_right = images[:, :, :n+1, -n:]\n",
    "        bottom_left = images[:, :, -n:, :n+1]\n",
    "        bottom_right = images[:, :, -n:, -n:]\n",
    "        top_combined = torch.cat([top_left, top_right], dim=-1)\n",
    "        bottom_combined = torch.cat([bottom_left, bottom_right], dim=-1)\n",
    "        all_together = torch.cat([top_combined, bottom_combined], dim=-2)\n",
    "    \n",
    "    else:\n",
    "        n = int(filter_size / 2)\n",
    "        top_left = images[:, :, :n, :n]\n",
    "        top_middle = torch.unsqueeze(0.5**0.5 * (images[:, :, :n, n] + images[:, :, :n, -n]), -1)\n",
    "        top_right = images[:, :, :n, -(n-1):]\n",
    "        middle_left = torch.unsqueeze(0.5**0.5 * (images[:, :, n, :n] + images[:, :, -n, :n]), -2)\n",
    "        middle_middle = torch.unsqueeze(torch.unsqueeze(0.5 * \n",
    "                                    (images[:, :, n, n] + images[:, :, n, -n] + images[:, :, -n, n] + images[:, :, -n, -n]), \n",
    "                                    -1), -1)\n",
    "        middle_right = torch.unsqueeze(0.5**0.5 * (images[:, :, n, -(n-1):] + images[:, :, -n, -(n-1):]), -2)\n",
    "        bottom_left = images[:, :, -(n-1):, :n]\n",
    "        bottom_middle = torch.unsqueeze(0.5 ** 0.5 * (images[:, :, -(n-1):, n] + images[:, :, -(n-1):, -n]), -1)\n",
    "        bottom_right = images[:, :, -(n-1):, -(n-1):]\n",
    "        top_combined = torch.cat([top_left, top_middle, top_right], dim=-1)\n",
    "        middle_combined = torch.cat([middle_left, middle_middle, middle_right], dim=-1)\n",
    "        bottom_combined = torch.cat([bottom_left, bottom_middle, bottom_right], dim=-1)\n",
    "        all_together = torch.cat([top_combined, middle_combined, bottom_combined], dim=-2)\n",
    "        \n",
    "    return all_together\n",
    "    \n",
    "\n",
    "class SpectralPool(Module):\n",
    "    def __init__(self, filter_size):\n",
    "        super(SpectralPool, self).__init__()\n",
    "        self.filter_size = torch.IntTensor(1).fill_(filter_size)\n",
    "        self.fft = fft.Fft2d()\n",
    "        self.ifft = fft.Ifft2d()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        in_re, in_im = self.fft(input, torch.zeros_like(input).cuda())\n",
    "        trans_re = _forward_spectral_pool(in_re, self.filter_size)\n",
    "        trans_im = _forward_spectral_pool(in_im, self.filter_size)\n",
    "        out_re, out_im = self.ifft(trans_re, trans_im)\n",
    "        \n",
    "        return out_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = SpectralPool(filter_size=16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = SpectralPool(filter_size=8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 8 * 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  train loss: 1.680  train acc: 0.395  val acc: 0.505\n",
      "epoch: 2  train loss: 1.308  train acc: 0.533  val acc: 0.568\n",
      "epoch: 3  train loss: 1.154  train acc: 0.589  val acc: 0.592\n",
      "epoch: 4  train loss: 1.048  train acc: 0.625  val acc: 0.628\n",
      "epoch: 5  train loss: 0.956  train acc: 0.660  val acc: 0.650\n",
      "epoch: 6  train loss: 0.893  train acc: 0.684  val acc: 0.648\n",
      "epoch: 7  train loss: 0.821  train acc: 0.710  val acc: 0.659\n",
      "epoch: 8  train loss: 0.746  train acc: 0.736  val acc: 0.650\n",
      "epoch: 9  train loss: 0.687  train acc: 0.757  val acc: 0.655\n",
      "epoch: 10  train loss: 0.627  train acc: 0.779  val acc: 0.667\n",
      "epoch: 11  train loss: 0.570  train acc: 0.796  val acc: 0.651\n",
      "epoch: 12  train loss: 0.511  train acc: 0.818  val acc: 0.667\n",
      "epoch: 13  train loss: 0.468  train acc: 0.834  val acc: 0.663\n",
      "epoch: 14  train loss: 0.425  train acc: 0.848  val acc: 0.671\n",
      "epoch: 15  train loss: 0.385  train acc: 0.862  val acc: 0.665\n",
      "epoch: 16  train loss: 0.358  train acc: 0.871  val acc: 0.664\n",
      "epoch: 17  train loss: 0.323  train acc: 0.883  val acc: 0.665\n",
      "epoch: 18  train loss: 0.292  train acc: 0.895  val acc: 0.650\n",
      "epoch: 19  train loss: 0.277  train acc: 0.899  val acc: 0.654\n",
      "epoch: 20  train loss: 0.245  train acc: 0.912  val acc: 0.655\n",
      "epoch: 21  train loss: 0.178  train acc: 0.938  val acc: 0.662\n",
      "epoch: 22  train loss: 0.152  train acc: 0.948  val acc: 0.659\n",
      "epoch: 23  train loss: 0.158  train acc: 0.943  val acc: 0.645\n",
      "epoch: 24  train loss: 0.157  train acc: 0.944  val acc: 0.648\n",
      "epoch: 25  train loss: 0.155  train acc: 0.945  val acc: 0.661\n",
      "epoch: 26  train loss: 0.142  train acc: 0.950  val acc: 0.644\n",
      "epoch: 27  train loss: 0.150  train acc: 0.948  val acc: 0.652\n",
      "epoch: 28  train loss: 0.135  train acc: 0.952  val acc: 0.658\n",
      "epoch: 29  train loss: 0.145  train acc: 0.949  val acc: 0.654\n",
      "epoch: 30  train loss: 0.138  train acc: 0.951  val acc: 0.653\n",
      "epoch: 31  train loss: 0.132  train acc: 0.952  val acc: 0.652\n",
      "epoch: 32  train loss: 0.132  train acc: 0.953  val acc: 0.655\n",
      "epoch: 33  train loss: 0.130  train acc: 0.954  val acc: 0.656\n",
      "epoch: 34  train loss: 0.136  train acc: 0.952  val acc: 0.654\n",
      "epoch: 35  train loss: 0.134  train acc: 0.952  val acc: 0.655\n",
      "epoch: 36  train loss: 0.117  train acc: 0.959  val acc: 0.659\n",
      "epoch: 37  train loss: 0.126  train acc: 0.956  val acc: 0.654\n",
      "epoch: 38  train loss: 0.127  train acc: 0.955  val acc: 0.645\n",
      "epoch: 39  train loss: 0.115  train acc: 0.960  val acc: 0.650\n",
      "epoch: 40  train loss: 0.115  train acc: 0.959  val acc: 0.644\n",
      "epoch: 41  train loss: 0.072  train acc: 0.976  val acc: 0.660\n",
      "epoch: 42  train loss: 0.048  train acc: 0.984  val acc: 0.653\n",
      "epoch: 43  train loss: 0.076  train acc: 0.974  val acc: 0.657\n",
      "epoch: 44  train loss: 0.088  train acc: 0.969  val acc: 0.651\n",
      "epoch: 45  train loss: 0.088  train acc: 0.969  val acc: 0.654\n",
      "epoch: 46  train loss: 0.088  train acc: 0.970  val acc: 0.651\n",
      "epoch: 47  train loss: 0.085  train acc: 0.970  val acc: 0.656\n",
      "epoch: 48  train loss: 0.083  train acc: 0.971  val acc: 0.656\n",
      "epoch: 49  train loss: 0.080  train acc: 0.972  val acc: 0.660\n",
      "epoch: 50  train loss: 0.085  train acc: 0.970  val acc: 0.642\n",
      "epoch: 51  train loss: 0.087  train acc: 0.971  val acc: 0.657\n",
      "epoch: 52  train loss: 0.087  train acc: 0.970  val acc: 0.657\n",
      "epoch: 53  train loss: 0.070  train acc: 0.976  val acc: 0.657\n",
      "epoch: 54  train loss: 0.080  train acc: 0.973  val acc: 0.641\n",
      "epoch: 55  train loss: 0.088  train acc: 0.969  val acc: 0.651\n",
      "epoch: 56  train loss: 0.086  train acc: 0.970  val acc: 0.656\n",
      "epoch: 57  train loss: 0.093  train acc: 0.967  val acc: 0.658\n",
      "epoch: 58  train loss: 0.080  train acc: 0.973  val acc: 0.660\n",
      "epoch: 59  train loss: 0.072  train acc: 0.975  val acc: 0.660\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain[:-4096], ytrain[:-4096])\n",
    "    val_gen = ImageGenerator(xtrain[-4096:], ytrain[-4096:])\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    val_generator = val_gen.next_batch_gen(batch_size)\n",
    "    \n",
    "    iters = int((xtrain.shape[0] - 4096) / batch_size)\n",
    "    val_iters = int(4096 / batch_size)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        start = time.time()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        train_loss = np.mean(loss_iter)\n",
    "        train_acc = np.sum(acc_iter) / (xtrain.shape[0] - 4096)\n",
    "        \n",
    "        # validation\n",
    "        val_iter = []\n",
    "        for itr in range(val_iters):\n",
    "            X_batch, y_batch = next(val_generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "            val_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        val_acc = np.sum(val_iter) / 4096\n",
    "        \n",
    "        print('epoch: %d  train loss: %.3f  train acc: %.3f  val acc: %.3f' % (epoch + 1, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # test the network\n",
    "    test_gen = ImageGenerator(xtest, ytest)\n",
    "    generator = test_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtest.shape[0] / batch_size)\n",
    "    test_iter = []\n",
    "    for itr in range(iters):\n",
    "        X_batch, y_batch = next(val_generator)\n",
    "        inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "        labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "        outputs = net.forward(inputs)\n",
    "            \n",
    "        _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "        test_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "    test_acc = np.sum(test_iter) / xtest.shape[0]\n",
    "        \n",
    "    print('test acc: %.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Internet broke down after epoch 59. Since it was really slow, I didn't retry it to finish 100 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = SpectralPool(filter_size=16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 160, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = SpectralPool(filter_size=8)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(160, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool3 = SpectralPool(filter_size=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(192, 192, kernel_size=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(192, 10, kernel_size=1, padding=0)\n",
    "        \n",
    "        self.avg = nn.AvgPool2d(4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.conv5(F.relu(self.conv4(x)))\n",
    "        \n",
    "        return torch.squeeze(self.avg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  train loss: 1.830  train acc: 0.319  val acc: 0.443\n",
      "epoch: 2  train loss: 1.452  train acc: 0.470  val acc: 0.501\n",
      "epoch: 3  train loss: 1.332  train acc: 0.516  val acc: 0.528\n",
      "epoch: 4  train loss: 1.238  train acc: 0.553  val acc: 0.558\n",
      "epoch: 5  train loss: 1.167  train acc: 0.581  val acc: 0.595\n",
      "epoch: 6  train loss: 1.102  train acc: 0.606  val acc: 0.613\n",
      "epoch: 7  train loss: 1.049  train acc: 0.628  val acc: 0.632\n",
      "epoch: 8  train loss: 1.014  train acc: 0.641  val acc: 0.644\n",
      "epoch: 9  train loss: 0.969  train acc: 0.656  val acc: 0.646\n",
      "epoch: 10  train loss: 0.944  train acc: 0.663  val acc: 0.684\n",
      "epoch: 11  train loss: 0.911  train acc: 0.678  val acc: 0.662\n",
      "epoch: 12  train loss: 0.885  train acc: 0.686  val acc: 0.698\n",
      "epoch: 13  train loss: 0.861  train acc: 0.696  val acc: 0.687\n",
      "epoch: 14  train loss: 0.828  train acc: 0.707  val acc: 0.685\n",
      "epoch: 15  train loss: 0.810  train acc: 0.714  val acc: 0.681\n",
      "epoch: 16  train loss: 0.791  train acc: 0.723  val acc: 0.707\n",
      "epoch: 17  train loss: 0.775  train acc: 0.727  val acc: 0.717\n",
      "epoch: 18  train loss: 0.749  train acc: 0.738  val acc: 0.708\n",
      "epoch: 19  train loss: 0.730  train acc: 0.744  val acc: 0.719\n",
      "epoch: 20  train loss: 0.719  train acc: 0.746  val acc: 0.709\n",
      "epoch: 21  train loss: 0.681  train acc: 0.760  val acc: 0.732\n",
      "epoch: 22  train loss: 0.666  train acc: 0.765  val acc: 0.733\n",
      "epoch: 23  train loss: 0.655  train acc: 0.769  val acc: 0.741\n",
      "epoch: 24  train loss: 0.645  train acc: 0.773  val acc: 0.733\n",
      "epoch: 25  train loss: 0.631  train acc: 0.778  val acc: 0.722\n",
      "epoch: 26  train loss: 0.617  train acc: 0.784  val acc: 0.717\n",
      "epoch: 27  train loss: 0.610  train acc: 0.784  val acc: 0.738\n",
      "epoch: 28  train loss: 0.592  train acc: 0.792  val acc: 0.744\n",
      "epoch: 29  train loss: 0.580  train acc: 0.797  val acc: 0.744\n",
      "epoch: 30  train loss: 0.570  train acc: 0.799  val acc: 0.751\n",
      "epoch: 31  train loss: 0.566  train acc: 0.801  val acc: 0.753\n",
      "epoch: 32  train loss: 0.557  train acc: 0.807  val acc: 0.748\n",
      "epoch: 33  train loss: 0.548  train acc: 0.809  val acc: 0.743\n",
      "epoch: 34  train loss: 0.538  train acc: 0.810  val acc: 0.758\n",
      "epoch: 35  train loss: 0.529  train acc: 0.815  val acc: 0.744\n",
      "epoch: 36  train loss: 0.519  train acc: 0.817  val acc: 0.748\n",
      "epoch: 37  train loss: 0.517  train acc: 0.817  val acc: 0.750\n",
      "epoch: 38  train loss: 0.505  train acc: 0.822  val acc: 0.741\n",
      "epoch: 39  train loss: 0.500  train acc: 0.826  val acc: 0.754\n",
      "epoch: 40  train loss: 0.498  train acc: 0.824  val acc: 0.740\n",
      "epoch: 41  train loss: 0.462  train acc: 0.837  val acc: 0.751\n",
      "epoch: 42  train loss: 0.457  train acc: 0.838  val acc: 0.764\n",
      "epoch: 43  train loss: 0.446  train acc: 0.842  val acc: 0.755\n",
      "epoch: 44  train loss: 0.439  train acc: 0.847  val acc: 0.755\n",
      "epoch: 45  train loss: 0.441  train acc: 0.845  val acc: 0.759\n",
      "epoch: 46  train loss: 0.431  train acc: 0.848  val acc: 0.751\n",
      "epoch: 47  train loss: 0.422  train acc: 0.852  val acc: 0.764\n",
      "epoch: 48  train loss: 0.418  train acc: 0.854  val acc: 0.752\n",
      "epoch: 49  train loss: 0.416  train acc: 0.853  val acc: 0.754\n",
      "epoch: 50  train loss: 0.408  train acc: 0.855  val acc: 0.747\n",
      "test acc: 0.745\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "total_epoch = 50\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain[:-4096], ytrain[:-4096])\n",
    "    val_gen = ImageGenerator(xtrain[-4096:], ytrain[-4096:])\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    val_generator = val_gen.next_batch_gen(batch_size)\n",
    "    \n",
    "    iters = int((xtrain.shape[0] - 4096) / batch_size)\n",
    "    val_iters = int(4096 / batch_size)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        start = time.time()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        train_loss = np.mean(loss_iter)\n",
    "        train_acc = np.sum(acc_iter) / (xtrain.shape[0] - 4096)\n",
    "        \n",
    "        # validation\n",
    "        val_iter = []\n",
    "        for itr in range(val_iters):\n",
    "            X_batch, y_batch = next(val_generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "            val_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        val_acc = np.sum(val_iter) / 4096\n",
    "        \n",
    "        print('epoch: %d  train loss: %.3f  train acc: %.3f  val acc: %.3f' % (epoch + 1, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # test the network\n",
    "    test_gen = ImageGenerator(xtest, ytest)\n",
    "    generator = test_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtest.shape[0] / batch_size)\n",
    "    test_iter = []\n",
    "    for itr in range(iters):\n",
    "        X_batch, y_batch = next(val_generator)\n",
    "        inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "        labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "        outputs = net.forward(inputs)\n",
    "            \n",
    "        _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "        test_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "    test_acc = np.sum(test_iter) / xtest.shape[0]\n",
    "        \n",
    "    print('test acc: %.3f' % (test_acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
