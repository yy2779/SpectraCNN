{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=1)\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from modules.utils import load_cifar10\n",
    "# from modules.cnn_with_spectral_parameterization import CNN_Spectral_Param\n",
    "# from modules.cnn_with_spectral_pooling import CNN_Spectral_Pool\n",
    "from modules.image_generator import ImageGenerator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already downloaded..\n",
      "getting batch 1\n"
     ]
    }
   ],
   "source": [
    "# In the interest of training time, we only used 1 of 5 cifar10 batches\n",
    "# The important part of the experiment is to compare the rates of convergence of training accuracy,\n",
    "# so subsetting the training dataset for both spectral and spatial models shouldn't impact\n",
    "# the relationship between their train accuracy convergences\n",
    "xtrain, ytrain, xtest, ytest = load_cifar10(1, channels_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 32, 32, 3), (10000,), (10000, 32, 32, 3), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rewrite tensorflow model to pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model in Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(generic_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 8 * 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    \n",
    "class deep_Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(deep_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.conv2 = nn.Conv2d(96, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pad1 = nn.ZeroPad2d((0, 1, 0, 1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(96, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.conv4 = nn.Conv2d(192, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.conv5 = nn.Conv2d(192, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pad2 = nn.ZeroPad2d((0, 1, 0, 1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(192, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.conv7 = nn.Conv2d(192, 10, kernel_size, padding=(kernel_size-1)//2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv2(F.relu(self.conv1(x))))\n",
    "        x = self.pool1(self.pad1(x))\n",
    "        x = F.relu(self.conv5(F.relu(self.conv4(F.relu(self.conv3(x))))))\n",
    "        x = self.pool2(self.pad2(x))\n",
    "        x = self.conv7(F.relu(self.conv6(x)))\n",
    "        for ax in [-2, -1]:\n",
    "            x = torch.mean(x, ax)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.750  acc: 0.140 \n",
      "epoch: 2  loss: 2.657  acc: 0.203 \n",
      "epoch: 3  loss: 2.711  acc: 0.175 \n",
      "epoch: 4  loss: 2.756  acc: 0.149 \n",
      "epoch: 5  loss: 2.685  acc: 0.141 \n",
      "epoch: 6  loss: 2.651  acc: 0.179 \n",
      "epoch: 7  loss: 2.643  acc: 0.182 \n",
      "epoch: 8  loss: 2.597  acc: 0.169 \n",
      "epoch: 9  loss: 2.520  acc: 0.192 \n",
      "epoch: 10  loss: 2.450  acc: 0.239 \n",
      "epoch: 11  loss: 2.503  acc: 0.238 \n",
      "epoch: 12  loss: 2.675  acc: 0.198 \n",
      "epoch: 13  loss: 2.553  acc: 0.236 \n",
      "epoch: 14  loss: 2.585  acc: 0.220 \n",
      "epoch: 15  loss: 2.618  acc: 0.214 \n",
      "epoch: 16  loss: 2.560  acc: 0.222 \n",
      "epoch: 17  loss: 2.509  acc: 0.212 \n",
      "epoch: 18  loss: 2.478  acc: 0.226 \n",
      "epoch: 19  loss: 2.478  acc: 0.247 \n",
      "epoch: 20  loss: 2.626  acc: 0.222 \n",
      "epoch: 21  loss: 2.585  acc: 0.249 \n",
      "epoch: 22  loss: 2.678  acc: 0.238 \n",
      "epoch: 23  loss: 2.543  acc: 0.229 \n",
      "epoch: 24  loss: 2.676  acc: 0.187 \n",
      "epoch: 25  loss: 2.657  acc: 0.189 \n",
      "epoch: 26  loss: 2.619  acc: 0.232 \n",
      "epoch: 27  loss: 2.632  acc: 0.222 \n",
      "epoch: 28  loss: 2.650  acc: 0.196 \n",
      "epoch: 29  loss: 2.622  acc: 0.239 \n",
      "epoch: 30  loss: 2.591  acc: 0.273 \n",
      "epoch: 31  loss: 2.538  acc: 0.296 \n",
      "epoch: 32  loss: 2.499  acc: 0.300 \n",
      "epoch: 33  loss: 2.459  acc: 0.317 \n",
      "epoch: 34  loss: 2.453  acc: 0.317 \n",
      "epoch: 35  loss: 2.436  acc: 0.331 \n",
      "epoch: 36  loss: 2.411  acc: 0.340 \n",
      "epoch: 37  loss: 2.404  acc: 0.341 \n",
      "epoch: 38  loss: 2.368  acc: 0.355 \n",
      "epoch: 39  loss: 2.403  acc: 0.354 \n",
      "epoch: 40  loss: 2.367  acc: 0.361 \n",
      "epoch: 41  loss: 2.451  acc: 0.342 \n",
      "epoch: 42  loss: 2.377  acc: 0.364 \n",
      "epoch: 43  loss: 2.416  acc: 0.373 \n",
      "epoch: 44  loss: 2.431  acc: 0.358 \n",
      "epoch: 45  loss: 2.404  acc: 0.348 \n",
      "epoch: 46  loss: 2.397  acc: 0.354 \n",
      "epoch: 47  loss: 2.391  acc: 0.364 \n",
      "epoch: 48  loss: 2.363  acc: 0.380 \n",
      "epoch: 49  loss: 2.386  acc: 0.387 \n",
      "epoch: 50  loss: 2.393  acc: 0.388 \n",
      "epoch: 51  loss: 2.357  acc: 0.395 \n",
      "epoch: 52  loss: 2.339  acc: 0.399 \n",
      "epoch: 53  loss: 2.359  acc: 0.391 \n",
      "epoch: 54  loss: 2.342  acc: 0.402 \n",
      "epoch: 55  loss: 2.333  acc: 0.415 \n",
      "epoch: 56  loss: 2.338  acc: 0.421 \n",
      "epoch: 57  loss: 2.330  acc: 0.429 \n",
      "epoch: 58  loss: 2.319  acc: 0.428 \n",
      "epoch: 59  loss: 2.310  acc: 0.425 \n",
      "epoch: 60  loss: 2.333  acc: 0.421 \n",
      "epoch: 61  loss: 2.317  acc: 0.438 \n",
      "epoch: 62  loss: 2.283  acc: 0.449 \n",
      "epoch: 63  loss: 2.313  acc: 0.442 \n",
      "epoch: 64  loss: 2.304  acc: 0.450 \n",
      "epoch: 65  loss: 2.299  acc: 0.452 \n",
      "epoch: 66  loss: 2.321  acc: 0.451 \n",
      "epoch: 67  loss: 2.297  acc: 0.464 \n",
      "epoch: 68  loss: 2.322  acc: 0.449 \n",
      "epoch: 69  loss: 2.327  acc: 0.447 \n",
      "epoch: 70  loss: 2.287  acc: 0.466 \n",
      "epoch: 71  loss: 2.300  acc: 0.462 \n",
      "epoch: 72  loss: 2.336  acc: 0.451 \n",
      "epoch: 73  loss: 2.306  acc: 0.464 \n",
      "epoch: 74  loss: 2.294  acc: 0.472 \n",
      "epoch: 75  loss: 2.315  acc: 0.464 \n",
      "epoch: 76  loss: 2.326  acc: 0.461 \n",
      "epoch: 77  loss: 2.305  acc: 0.472 \n",
      "epoch: 78  loss: 2.298  acc: 0.472 \n",
      "epoch: 79  loss: 2.301  acc: 0.472 \n",
      "epoch: 80  loss: 2.283  acc: 0.490 \n",
      "epoch: 81  loss: 2.276  acc: 0.500 \n",
      "epoch: 82  loss: 2.289  acc: 0.494 \n",
      "epoch: 83  loss: 2.289  acc: 0.493 \n",
      "epoch: 84  loss: 2.276  acc: 0.497 \n",
      "epoch: 85  loss: 2.277  acc: 0.501 \n",
      "epoch: 86  loss: 2.266  acc: 0.502 \n",
      "epoch: 87  loss: 2.266  acc: 0.501 \n",
      "epoch: 88  loss: 2.286  acc: 0.495 \n",
      "epoch: 89  loss: 2.283  acc: 0.504 \n",
      "epoch: 90  loss: 2.262  acc: 0.517 \n",
      "epoch: 91  loss: 2.251  acc: 0.518 \n",
      "epoch: 92  loss: 2.262  acc: 0.515 \n",
      "epoch: 93  loss: 2.275  acc: 0.514 \n",
      "epoch: 94  loss: 2.257  acc: 0.523 \n",
      "epoch: 95  loss: 2.233  acc: 0.536 \n",
      "epoch: 96  loss: 2.234  acc: 0.537 \n",
      "epoch: 97  loss: 2.261  acc: 0.526 \n",
      "epoch: 98  loss: 2.270  acc: 0.525 \n",
      "epoch: 99  loss: 2.229  acc: 0.543 \n",
      "epoch: 100  loss: 2.214  acc: 0.549 \n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 200\n",
    "learning_rate = 1e-5\n",
    "l2norm = 0.01\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = generic_Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain, ytrain)\n",
    "#     img_gen.translate(shift_height=-2, shift_width=0)\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtrain.shape[0] / batch_size)\n",
    "    \n",
    "    itr = 0\n",
    "    for epoch in range(total_epoch):\n",
    "#         if epc % 4 == 0 or epc % 4 == 1:\n",
    "#             img_gen.translate(shift_height=2, shift_width=0)\n",
    "#         elif epc % 4 == 2 or epc % 4 == 3:\n",
    "#             img_gen.translate(shift_height=-2, shift_width=0)\n",
    "        \n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            itr += 1\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch.transpose(0,3,1,2)).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            t = -1\n",
    "            for w in net.parameters():\n",
    "                t += 1\n",
    "                if t % 2 == 1: # because parameters contain both bias and weights, only need weights here\n",
    "                    continue\n",
    "                loss += w.norm(2) * l2norm\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum() / batch_size)\n",
    "        \n",
    "        ave_loss = np.mean(loss_iter)\n",
    "        ave_acc = np.mean(acc_iter)\n",
    "        print('epoch: %d  loss: %.3f  acc: %.3f ' % (epoch + 1, ave_loss, ave_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.751  acc: 0.098 \n",
      "epoch: 2  loss: 2.743  acc: 0.098 \n",
      "epoch: 3  loss: 2.735  acc: 0.098 \n",
      "epoch: 4  loss: 2.726  acc: 0.113 \n",
      "epoch: 5  loss: 2.716  acc: 0.104 \n",
      "epoch: 6  loss: 2.706  acc: 0.107 \n",
      "epoch: 7  loss: 2.694  acc: 0.117 \n",
      "epoch: 8  loss: 2.675  acc: 0.131 \n",
      "epoch: 9  loss: 2.638  acc: 0.157 \n",
      "epoch: 10  loss: 2.580  acc: 0.171 \n",
      "epoch: 11  loss: 2.707  acc: 0.145 \n",
      "epoch: 12  loss: 2.805  acc: 0.103 \n",
      "epoch: 13  loss: 2.668  acc: 0.105 \n",
      "epoch: 14  loss: 2.655  acc: 0.102 \n",
      "epoch: 15  loss: 2.650  acc: 0.099 \n",
      "epoch: 16  loss: 2.644  acc: 0.098 \n",
      "epoch: 17  loss: 2.637  acc: 0.098 \n",
      "epoch: 18  loss: 2.624  acc: 0.098 \n",
      "epoch: 19  loss: 2.598  acc: 0.098 \n",
      "epoch: 20  loss: 2.609  acc: 0.098 \n",
      "epoch: 21  loss: 2.575  acc: 0.098 \n",
      "epoch: 22  loss: 2.568  acc: 0.127 \n",
      "epoch: 23  loss: 2.563  acc: 0.142 \n",
      "epoch: 24  loss: 2.528  acc: 0.146 \n",
      "epoch: 25  loss: 2.538  acc: 0.131 \n",
      "epoch: 26  loss: 2.525  acc: 0.141 \n",
      "epoch: 27  loss: 2.536  acc: 0.143 \n",
      "epoch: 28  loss: 2.559  acc: 0.135 \n",
      "epoch: 29  loss: 2.564  acc: 0.128 \n",
      "epoch: 30  loss: 2.559  acc: 0.129 \n",
      "epoch: 31  loss: 2.547  acc: 0.133 \n",
      "epoch: 32  loss: 2.528  acc: 0.140 \n",
      "epoch: 33  loss: 2.507  acc: 0.150 \n",
      "epoch: 34  loss: 2.525  acc: 0.150 \n",
      "epoch: 35  loss: 2.506  acc: 0.149 \n",
      "epoch: 36  loss: 2.485  acc: 0.155 \n",
      "epoch: 37  loss: 2.510  acc: 0.153 \n",
      "epoch: 38  loss: 2.522  acc: 0.150 \n",
      "epoch: 39  loss: 2.519  acc: 0.153 \n",
      "epoch: 40  loss: 2.505  acc: 0.154 \n",
      "epoch: 41  loss: 2.483  acc: 0.150 \n",
      "epoch: 42  loss: 2.474  acc: 0.142 \n",
      "epoch: 43  loss: 2.498  acc: 0.136 \n",
      "epoch: 44  loss: 2.503  acc: 0.143 \n",
      "epoch: 45  loss: 2.464  acc: 0.154 \n",
      "epoch: 46  loss: 2.459  acc: 0.164 \n",
      "epoch: 47  loss: 2.479  acc: 0.162 \n",
      "epoch: 48  loss: 2.492  acc: 0.159 \n",
      "epoch: 49  loss: 2.492  acc: 0.160 \n",
      "epoch: 50  loss: 2.481  acc: 0.159 \n",
      "epoch: 51  loss: 2.458  acc: 0.165 \n",
      "epoch: 52  loss: 2.425  acc: 0.170 \n",
      "epoch: 53  loss: 2.400  acc: 0.169 \n",
      "epoch: 54  loss: 2.417  acc: 0.161 \n",
      "epoch: 55  loss: 2.424  acc: 0.162 \n",
      "epoch: 56  loss: 2.376  acc: 0.170 \n",
      "epoch: 57  loss: 2.387  acc: 0.190 \n",
      "epoch: 58  loss: 2.434  acc: 0.181 \n",
      "epoch: 59  loss: 2.420  acc: 0.183 \n",
      "epoch: 60  loss: 2.363  acc: 0.196 \n",
      "epoch: 61  loss: 2.354  acc: 0.203 \n",
      "epoch: 62  loss: 2.398  acc: 0.197 \n",
      "epoch: 63  loss: 2.405  acc: 0.195 \n",
      "epoch: 64  loss: 2.369  acc: 0.201 \n",
      "epoch: 65  loss: 2.364  acc: 0.200 \n",
      "epoch: 66  loss: 2.387  acc: 0.197 \n",
      "epoch: 67  loss: 2.409  acc: 0.191 \n",
      "epoch: 68  loss: 2.411  acc: 0.188 \n",
      "epoch: 69  loss: 2.394  acc: 0.189 \n",
      "epoch: 70  loss: 2.372  acc: 0.197 \n",
      "epoch: 71  loss: 2.358  acc: 0.200 \n",
      "epoch: 72  loss: 2.359  acc: 0.203 \n",
      "epoch: 73  loss: 2.373  acc: 0.200 \n",
      "epoch: 74  loss: 2.372  acc: 0.200 \n",
      "epoch: 75  loss: 2.344  acc: 0.207 \n",
      "epoch: 76  loss: 2.327  acc: 0.208 \n",
      "epoch: 77  loss: 2.349  acc: 0.207 \n",
      "epoch: 78  loss: 2.375  acc: 0.199 \n",
      "epoch: 79  loss: 2.358  acc: 0.202 \n",
      "epoch: 80  loss: 2.328  acc: 0.215 \n",
      "epoch: 81  loss: 2.334  acc: 0.222 \n",
      "epoch: 82  loss: 2.366  acc: 0.222 \n",
      "epoch: 83  loss: 2.370  acc: 0.228 \n",
      "epoch: 84  loss: 2.342  acc: 0.229 \n",
      "epoch: 85  loss: 2.323  acc: 0.227 \n",
      "epoch: 86  loss: 2.332  acc: 0.218 \n",
      "epoch: 87  loss: 2.352  acc: 0.211 \n",
      "epoch: 88  loss: 2.359  acc: 0.208 \n",
      "epoch: 89  loss: 2.341  acc: 0.213 \n",
      "epoch: 90  loss: 2.315  acc: 0.218 \n",
      "epoch: 91  loss: 2.313  acc: 0.219 \n",
      "epoch: 92  loss: 2.336  acc: 0.216 \n",
      "epoch: 93  loss: 2.340  acc: 0.217 \n",
      "epoch: 94  loss: 2.313  acc: 0.226 \n",
      "epoch: 95  loss: 2.292  acc: 0.228 \n",
      "epoch: 96  loss: 2.308  acc: 0.222 \n",
      "epoch: 97  loss: 2.335  acc: 0.217 \n",
      "epoch: 98  loss: 2.328  acc: 0.218 \n",
      "epoch: 99  loss: 2.299  acc: 0.227 \n",
      "epoch: 100  loss: 2.292  acc: 0.235 \n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 200\n",
    "learning_rate = 5e-6\n",
    "l2norm = 0.01\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = deep_Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain, ytrain)\n",
    "#     img_gen.translate(shift_height=-2, shift_width=0)\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtrain.shape[0] / batch_size)\n",
    "    \n",
    "    itr = 0\n",
    "    for epoch in range(total_epoch):\n",
    "#         if epc % 4 == 0 or epc % 4 == 1:\n",
    "#             img_gen.translate(shift_height=2, shift_width=0)\n",
    "#         elif epc % 4 == 2 or epc % 4 == 3:\n",
    "#             img_gen.translate(shift_height=-2, shift_width=0)\n",
    "        \n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            itr += 1\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch.transpose(0,3,1,2)).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            t = -1\n",
    "            for w in net.parameters():\n",
    "                t += 1\n",
    "                if t % 2 == 1:  # because parameters contain both bias and weights, only need weights here\n",
    "                    continue\n",
    "                loss += w.norm(2) * l2norm\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum() / batch_size)\n",
    "        \n",
    "        ave_loss = np.mean(loss_iter)\n",
    "        ave_acc = np.mean(acc_iter)\n",
    "        print('epoch: %d  loss: %.3f  acc: %.3f ' % (epoch + 1, ave_loss, ave_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spectral pooling with CPU\n",
    "## Spectral Pooling Layer with Numpy\n",
    "### Should be very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _forward_spectral_pool(images, pool_stride):\n",
    "    assert pool_stride >= 3\n",
    "    assert images.shape[-1] == images.shape[-2] and images.shape[-1] >= 3\n",
    "    \n",
    "    n = (images.shape[-1] - 1) // pool_stride + 1\n",
    "    \n",
    "    top_left = images[:, :, :n, :n]\n",
    "    top_right = images[:, :, :n, -n:]\n",
    "    bottom_left = images[:, :, -n:, :n]\n",
    "    bottom_right = images[:, :, -n:, -n:]\n",
    "    \n",
    "    top_combined = np.concatenate([top_left, top_right], axis=-1)\n",
    "    bottom_combined = np.concatenate([bottom_left, bottom_right], axis=-1)\n",
    "    all_together = np.concatenate([top_combined, bottom_combined], axis=-2)\n",
    "    \n",
    "    return all_together\n",
    "\n",
    "def _backward_spectral_pool(grad, origin_shape):\n",
    "    m = grad.shape[-1]\n",
    "    assert m%2 == 0\n",
    "    \n",
    "    pad1 = np.zeros((grad.shape[0], grad.shape[1], m//2, origin_shape-m))\n",
    "    pad2 = np.zeros((grad.shape[0], grad.shape[1], origin_shape-m, m//2))\n",
    "    pad3 = np.zeros((grad.shape[0], grad.shape[1], origin_shape-m, origin_shape-m))\n",
    "    \n",
    "    top_left = grad[:, :, :m//2, :m//2]\n",
    "    top_right = grad[:, :, :m//2, -m//2:]\n",
    "    bottom_left = grad[:, :, -m//2:, :m//2]\n",
    "    bottom_right = grad[:, :, -m//2:, -m//2:]\n",
    "    \n",
    "    top_combined = np.concatenate([top_left, pad1, top_right], axis=-1)\n",
    "    pad_combined = np.concatenate([pad2, pad3, pad2], axis=-1)\n",
    "    bottom_combined = np.concatenate([bottom_left, pad1, bottom_right], axis=-1)\n",
    "    all_together = np.concatenate([top_combined, pad_combined, bottom_combined], axis=-2)\n",
    "    \n",
    "    return all_together\n",
    "    \n",
    "\n",
    "class Poolfunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, pool_stride):\n",
    "        ctx.save_for_backward(input)\n",
    "        im_fft = np.fft.fft2(input.cpu().numpy())\n",
    "        im_transformed = _forward_spectral_pool(im_fft, pool_stride.numpy()[0])\n",
    "        im_out = np.real(np.fft.ifft2(im_transformed))\n",
    "        return torch.FloatTensor(im_out).cuda()\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors[0]\n",
    "        \n",
    "        grad_fft = np.fft.fft2(grad_output.data.cpu().numpy())\n",
    "        grad_transformed = _backward_spectral_pool(grad_fft, input.cpu().numpy().shape[-1])\n",
    "        grad_out = np.real(np.fft.ifft2(grad_transformed))\n",
    "        return Variable(torch.FloatTensor(grad_out).cuda(), volatile=True), None\n",
    "\n",
    "class SpectralPool(Module):\n",
    "    def __init__(self, pool_stride):\n",
    "        super(SpectralPool, self).__init__()\n",
    "        self.pool_stride = torch.IntTensor(1).fill_(pool_stride)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return Poolfunction.apply(input, self.pool_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify the spectral pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      " -2.5425  0.8660  1.2152  1.2327\n",
      "  3.6477 -2.2184 -1.2238 -3.3269\n",
      " -0.2555  3.1346 -3.7369 -1.6330\n",
      "  0.3812 -2.9214 -0.7644  1.8675\n",
      "[torch.cuda.FloatTensor of size 1x1x4x4 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0403  0.0379 -0.0379  0.1785 -0.2391  0.0461  0.1493 -0.0945\n",
      "  0.0114 -0.0549  0.1233 -0.2526  0.2788 -0.1107 -0.0652  0.0698\n",
      " -0.0177  0.0607 -0.1452  0.2249 -0.2500  0.2026 -0.1137  0.0384\n",
      " -0.2204  0.2215 -0.0523 -0.1505  0.3179 -0.3897  0.2739 -0.0005\n",
      "  0.2875 -0.3652  0.2104  0.0462 -0.2943  0.4285 -0.3303  0.0172\n",
      "  0.1316  0.0126 -0.0935  0.0656  0.0446 -0.1746  0.2584 -0.2447\n",
      " -0.5155  0.3683 -0.0870 -0.0928  0.1365 -0.0894 -0.0917  0.3716\n",
      "  0.3634 -0.2810  0.0821 -0.0194  0.0056  0.0872 -0.0808 -0.1572\n",
      "[torch.cuda.FloatTensor of size 1x1x8x8 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test Spectral Pooling module\n",
    "input = Variable(torch.randn(1, 1, 8, 8).cuda(), requires_grad=True)\n",
    "module = SpectralPool(pool_stride=4).cuda()\n",
    "result = module(input)\n",
    "print(result.data)\n",
    "result.backward(torch.randn(result.size()).cuda())\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Numpy Spectral Pool with simple model: generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generic_Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(generic_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = SpectralPool(pool_stride=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = SpectralPool(pool_stride=4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 8 * 8 * 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.764  acc: 0.094 \n",
      "epoch: 2  loss: 2.736  acc: 0.134 \n",
      "epoch: 3  loss: 2.705  acc: 0.128 \n",
      "epoch: 4  loss: 2.675  acc: 0.124 \n",
      "epoch: 5  loss: 2.645  acc: 0.127 \n",
      "epoch: 6  loss: 2.618  acc: 0.131 \n",
      "epoch: 7  loss: 2.591  acc: 0.131 \n",
      "epoch: 8  loss: 2.568  acc: 0.146 \n",
      "epoch: 9  loss: 2.546  acc: 0.150 \n",
      "epoch: 10  loss: 2.528  acc: 0.156 \n"
     ]
    }
   ],
   "source": [
    "# Really slow\n",
    "\n",
    "kernel_size = 3\n",
    "batch_size = 200\n",
    "learning_rate = 1e-5\n",
    "l2norm = 0.01\n",
    "total_epoch = 10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = generic_Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain, ytrain)\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtrain.shape[0] / batch_size)\n",
    "    \n",
    "    itr = 0\n",
    "    for epoch in range(total_epoch):\n",
    "        \n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            itr += 1\n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch.transpose(0,3,1,2)).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            t = -1\n",
    "            for w in net.parameters():\n",
    "                t += 1\n",
    "                if t % 2 == 1:  # because parameters contain both bias and weights, only need weights here\n",
    "                    continue\n",
    "                loss += w.norm(2) * l2norm\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum() / batch_size)\n",
    "        \n",
    "        ave_loss = np.mean(loss_iter)\n",
    "        ave_acc = np.mean(acc_iter)\n",
    "        print('epoch: %d  loss: %.3f  acc: %.3f ' % (epoch + 1, ave_loss, ave_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spectral pooling with GPU\n",
    "## Spectral Pooling Layer with Pytorch-fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_fft.fft.autograd as fft\n",
    "\n",
    "def _forward_spectral_pool(images, pool_stride):\n",
    "    assert (torch.gt(pool_stride, 3)).all()\n",
    "    assert images.size()[-1] == images.size()[-2] and images.size()[-1] >= 3\n",
    "    \n",
    "    n = int((images.size()[-1] - 1) / pool_stride + 1)\n",
    "    \n",
    "    top_left = images[:, :, :n, :n]\n",
    "    top_right = images[:, :, :n, -n:]\n",
    "    bottom_left = images[:, :, -n:, :n]\n",
    "    bottom_right = images[:, :, -n:, -n:]\n",
    "    \n",
    "    top_combined = torch.cat([top_left, top_right], dim=-1)\n",
    "    bottom_combined = torch.cat([bottom_left, bottom_right], dim=-1)\n",
    "    all_together = torch.cat([top_combined, bottom_combined], dim=-2)\n",
    "    \n",
    "    return all_together\n",
    "    \n",
    "\n",
    "class SpectralPool(Module):\n",
    "    def __init__(self, pool_stride):\n",
    "        super(SpectralPool, self).__init__()\n",
    "        self.pool_stride = torch.IntTensor(1).fill_(pool_stride)\n",
    "        self.fft = fft.Fft2d()\n",
    "        self.ifft = fft.Ifft2d()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        in_re, in_im = self.fft(input, torch.zeros_like(input).cuda())\n",
    "        trans_re = _forward_spectral_pool(in_re, self.pool_stride)\n",
    "        trans_im = _forward_spectral_pool(in_im, self.pool_stride)\n",
    "        out_re, _ = self.ifft(trans_re, trans_im)\n",
    "        \n",
    "        return out_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pytorch-fft Spectral Pool with simple model: generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(generic_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = SpectralPool(pool_stride=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = SpectralPool(pool_stride=4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*192, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 8 * 8 * 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.679  acc: 0.169 \n",
      "epoch: 2  loss: 2.684  acc: 0.207 \n",
      "epoch: 3  loss: 2.609  acc: 0.215 \n",
      "epoch: 4  loss: 2.584  acc: 0.199 \n",
      "epoch: 5  loss: 2.519  acc: 0.235 \n",
      "epoch: 6  loss: 2.423  acc: 0.283 \n",
      "epoch: 7  loss: 2.428  acc: 0.294 \n",
      "epoch: 8  loss: 2.436  acc: 0.280 \n",
      "epoch: 9  loss: 2.408  acc: 0.272 \n",
      "epoch: 10  loss: 2.465  acc: 0.249 \n",
      "epoch: 11  loss: 2.325  acc: 0.308 \n",
      "epoch: 12  loss: 2.295  acc: 0.343 \n",
      "epoch: 13  loss: 2.266  acc: 0.350 \n",
      "epoch: 14  loss: 2.349  acc: 0.330 \n",
      "epoch: 15  loss: 2.271  acc: 0.340 \n",
      "epoch: 16  loss: 2.323  acc: 0.321 \n",
      "epoch: 17  loss: 2.264  acc: 0.352 \n",
      "epoch: 18  loss: 2.276  acc: 0.344 \n",
      "epoch: 19  loss: 2.287  acc: 0.349 \n",
      "epoch: 20  loss: 2.244  acc: 0.376 \n",
      "epoch: 21  loss: 2.197  acc: 0.398 \n",
      "epoch: 22  loss: 2.183  acc: 0.414 \n",
      "epoch: 23  loss: 2.172  acc: 0.423 \n",
      "epoch: 24  loss: 2.150  acc: 0.435 \n",
      "epoch: 25  loss: 2.207  acc: 0.414 \n",
      "epoch: 26  loss: 2.140  acc: 0.441 \n",
      "epoch: 27  loss: 2.199  acc: 0.423 \n",
      "epoch: 28  loss: 2.125  acc: 0.453 \n",
      "epoch: 29  loss: 2.141  acc: 0.449 \n",
      "epoch: 30  loss: 2.142  acc: 0.455 \n",
      "epoch: 31  loss: 2.124  acc: 0.468 \n",
      "epoch: 32  loss: 2.165  acc: 0.457 \n",
      "epoch: 33  loss: 2.128  acc: 0.470 \n",
      "epoch: 34  loss: 2.140  acc: 0.471 \n",
      "epoch: 35  loss: 2.149  acc: 0.454 \n",
      "epoch: 36  loss: 2.153  acc: 0.469 \n",
      "epoch: 37  loss: 2.201  acc: 0.463 \n",
      "epoch: 38  loss: 2.127  acc: 0.493 \n",
      "epoch: 39  loss: 2.128  acc: 0.491 \n",
      "epoch: 40  loss: 2.175  acc: 0.482 \n",
      "epoch: 41  loss: 2.141  acc: 0.494 \n",
      "epoch: 42  loss: 2.128  acc: 0.493 \n",
      "epoch: 43  loss: 2.143  acc: 0.497 \n",
      "epoch: 44  loss: 2.131  acc: 0.512 \n",
      "epoch: 45  loss: 2.076  acc: 0.530 \n",
      "epoch: 46  loss: 2.125  acc: 0.518 \n",
      "epoch: 47  loss: 2.083  acc: 0.536 \n",
      "epoch: 48  loss: 2.050  acc: 0.551 \n",
      "epoch: 49  loss: 2.050  acc: 0.554 \n",
      "epoch: 50  loss: 2.056  acc: 0.551 \n",
      "epoch: 51  loss: 2.044  acc: 0.558 \n",
      "epoch: 52  loss: 2.034  acc: 0.570 \n",
      "epoch: 53  loss: 2.037  acc: 0.576 \n",
      "epoch: 54  loss: 2.016  acc: 0.590 \n",
      "epoch: 55  loss: 2.027  acc: 0.585 \n",
      "epoch: 56  loss: 2.005  acc: 0.597 \n",
      "epoch: 57  loss: 1.991  acc: 0.609 \n",
      "epoch: 58  loss: 1.981  acc: 0.616 \n",
      "epoch: 59  loss: 1.978  acc: 0.617 \n",
      "epoch: 60  loss: 1.966  acc: 0.625 \n",
      "epoch: 61  loss: 1.966  acc: 0.629 \n",
      "epoch: 62  loss: 1.954  acc: 0.640 \n",
      "epoch: 63  loss: 1.948  acc: 0.645 \n",
      "epoch: 64  loss: 1.948  acc: 0.646 \n",
      "epoch: 65  loss: 1.935  acc: 0.657 \n",
      "epoch: 66  loss: 1.944  acc: 0.657 \n",
      "epoch: 67  loss: 1.923  acc: 0.667 \n",
      "epoch: 68  loss: 1.951  acc: 0.660 \n",
      "epoch: 69  loss: 1.922  acc: 0.668 \n",
      "epoch: 70  loss: 1.940  acc: 0.671 \n",
      "epoch: 71  loss: 1.948  acc: 0.667 \n",
      "epoch: 72  loss: 1.933  acc: 0.679 \n",
      "epoch: 73  loss: 1.949  acc: 0.672 \n",
      "epoch: 74  loss: 1.962  acc: 0.672 \n",
      "epoch: 75  loss: 1.930  acc: 0.691 \n",
      "epoch: 76  loss: 1.946  acc: 0.686 \n",
      "epoch: 77  loss: 1.982  acc: 0.675 \n",
      "epoch: 78  loss: 1.948  acc: 0.689 \n",
      "epoch: 79  loss: 1.928  acc: 0.698 \n",
      "epoch: 80  loss: 1.929  acc: 0.701 \n",
      "epoch: 81  loss: 1.923  acc: 0.711 \n",
      "epoch: 82  loss: 1.890  acc: 0.723 \n",
      "epoch: 83  loss: 1.945  acc: 0.710 \n",
      "epoch: 84  loss: 1.940  acc: 0.720 \n",
      "epoch: 85  loss: 1.865  acc: 0.749 \n",
      "epoch: 86  loss: 1.870  acc: 0.749 \n",
      "epoch: 87  loss: 1.927  acc: 0.728 \n",
      "epoch: 88  loss: 1.908  acc: 0.733 \n",
      "epoch: 89  loss: 1.892  acc: 0.742 \n",
      "epoch: 90  loss: 1.885  acc: 0.754 \n",
      "epoch: 91  loss: 1.856  acc: 0.769 \n",
      "epoch: 92  loss: 1.861  acc: 0.769 \n",
      "epoch: 93  loss: 1.883  acc: 0.763 \n",
      "epoch: 94  loss: 1.859  acc: 0.770 \n",
      "epoch: 95  loss: 1.813  acc: 0.793 \n",
      "epoch: 96  loss: 1.840  acc: 0.787 \n",
      "epoch: 97  loss: 1.843  acc: 0.787 \n",
      "epoch: 98  loss: 1.807  acc: 0.808 \n",
      "epoch: 99  loss: 1.796  acc: 0.812 \n",
      "epoch: 100  loss: 1.815  acc: 0.807 \n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 200\n",
    "learning_rate = 1e-5\n",
    "l2norm = 0.01\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = generic_Net(kernel_size).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain, ytrain)\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtrain.shape[0] / batch_size)\n",
    "    \n",
    "    itr = 0\n",
    "    for epoch in range(total_epoch):\n",
    "        \n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            itr += 1\n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch.transpose(0,3,1,2)).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            t = -1\n",
    "            for w in net.parameters():\n",
    "                t += 1\n",
    "                if t % 2 == 1:  # because parameters contain both bias and weights, only need weights here\n",
    "                    continue\n",
    "                loss += w.norm(2) * l2norm\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum() / batch_size)\n",
    "        \n",
    "        ave_loss = np.mean(loss_iter)\n",
    "        ave_acc = np.mean(acc_iter)\n",
    "        print('epoch: %d  loss: %.3f  acc: %.3f ' % (epoch + 1, ave_loss, ave_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
