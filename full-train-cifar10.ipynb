{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=1)\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from modules.utils import load_cifar10\n",
    "# from modules.cnn_with_spectral_parameterization import CNN_Spectral_Param\n",
    "# from modules.cnn_with_spectral_pooling import CNN_Spectral_Pool\n",
    "from modules.image_generator import ImageGenerator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import pytorch_fft.fft.autograd as fft\n",
    "\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already downloaded..\n",
      "getting batch 1\n",
      "getting batch 2\n",
      "getting batch 3\n",
      "getting batch 4\n",
      "getting batch 5\n"
     ]
    }
   ],
   "source": [
    "# In the interest of training time, we only used 1 of 5 cifar10 batches\n",
    "# The important part of the experiment is to compare the rates of convergence of training accuracy,\n",
    "# so subsetting the training dataset for both spectral and spatial models shouldn't impact\n",
    "# the relationship between their train accuracy convergences\n",
    "xtrain, ytrain, xtest, ytest = load_cifar10(5, channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (50000,), (10000, 3, 32, 32), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spectral Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_spectral_pool(images, filter_size):\n",
    "    assert (torch.ge(filter_size, 3)).all()\n",
    "    assert images.size()[-1] == images.size()[-2] and images.size()[-1] >= 3\n",
    "    \n",
    "    if int(filter_size) % 2 == 1:\n",
    "        n = int((filter_size - 1)/2)\n",
    "        top_left = images[:, :, :n+1, :n+1]\n",
    "        top_right = images[:, :, :n+1, -n:]\n",
    "        bottom_left = images[:, :, -n:, :n+1]\n",
    "        bottom_right = images[:, :, -n:, -n:]\n",
    "        top_combined = torch.cat([top_left, top_right], dim=-1)\n",
    "        bottom_combined = torch.cat([bottom_left, bottom_right], dim=-1)\n",
    "        all_together = torch.cat([top_combined, bottom_combined], dim=-2)\n",
    "    \n",
    "    else:\n",
    "        n = int(filter_size / 2)\n",
    "        top_left = images[:, :, :n, :n]\n",
    "        top_middle = torch.unsqueeze(0.5**0.5 * (images[:, :, :n, n] + images[:, :, :n, -n]), -1)\n",
    "        top_right = images[:, :, :n, -(n-1):]\n",
    "        middle_left = torch.unsqueeze(0.5**0.5 * (images[:, :, n, :n] + images[:, :, -n, :n]), -2)\n",
    "        middle_middle = torch.unsqueeze(torch.unsqueeze(0.5 * \n",
    "                                    (images[:, :, n, n] + images[:, :, n, -n] + images[:, :, -n, n] + images[:, :, -n, -n]), \n",
    "                                    -1), -1)\n",
    "        middle_right = torch.unsqueeze(0.5**0.5 * (images[:, :, n, -(n-1):] + images[:, :, -n, -(n-1):]), -2)\n",
    "        bottom_left = images[:, :, -(n-1):, :n]\n",
    "        bottom_middle = torch.unsqueeze(0.5 ** 0.5 * (images[:, :, -(n-1):, n] + images[:, :, -(n-1):, -n]), -1)\n",
    "        bottom_right = images[:, :, -(n-1):, -(n-1):]\n",
    "        top_combined = torch.cat([top_left, top_middle, top_right], dim=-1)\n",
    "        middle_combined = torch.cat([middle_left, middle_middle, middle_right], dim=-1)\n",
    "        bottom_combined = torch.cat([bottom_left, bottom_middle, bottom_right], dim=-1)\n",
    "        all_together = torch.cat([top_combined, middle_combined, bottom_combined], dim=-2)\n",
    "        \n",
    "    return all_together\n",
    "    \n",
    "\n",
    "class SpectralPool(Module):\n",
    "    def __init__(self, filter_size):\n",
    "        super(SpectralPool, self).__init__()\n",
    "        self.filter_size = torch.IntTensor(1).fill_(filter_size)\n",
    "        self.fft = fft.Fft2d()\n",
    "        self.ifft = fft.Ifft2d()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        in_re, in_im = self.fft(input, torch.zeros_like(input).cuda())\n",
    "        trans_re = _forward_spectral_pool(in_re, self.filter_size)\n",
    "        trans_im = _forward_spectral_pool(in_im, self.filter_size)\n",
    "        out_re, out_im = self.ifft(trans_re, trans_im)\n",
    "        \n",
    "        return out_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Layer with Spectral Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralParam(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(SpectralParam, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.ifft = fft.Ifft2d()\n",
    "        \n",
    "        weight = torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).cuda()\n",
    "        nn.init.xavier_uniform(weight)\n",
    "        weight_re, weight_im = fft.fft2(weight, torch.zeros_like(weight).cuda())\n",
    "        \n",
    "        self.weight_re = nn.Parameter(weight_re, requires_grad=True)\n",
    "        self.weight_im = nn.Parameter(weight_im, requires_grad=True)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels), requires_grad=True)\n",
    "            nn.init.normal(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        weight, _ = self.ifft(self.weight_re, self.weight_im)\n",
    "#         weight, _ = self.ifft(self.weight_re, torch.zeros_like(self.weight_re).cuda())\n",
    "        result = F.conv2d(input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Spectral CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SpectralParam(3, 128, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool1 = SpectralPool(filter_size=25)\n",
    "        \n",
    "        self.conv2 = SpectralParam(128, 160, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool2 = SpectralPool(filter_size=19)\n",
    "        \n",
    "        self.conv3 = SpectralParam(160, 192, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool3 = SpectralPool(filter_size=15)\n",
    "        \n",
    "        self.conv4 = SpectralParam(192, 224, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool4 = SpectralPool(filter_size=11)\n",
    "        \n",
    "        self.conv5 = SpectralParam(224, 256, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool5 = SpectralPool(filter_size=8)\n",
    "        \n",
    "        self.conv6 = SpectralParam(256, 288, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.pool6 = SpectralPool(filter_size=4)\n",
    "        \n",
    "        self.conv7 = SpectralParam(288, 288, kernel_size=1, padding=0)\n",
    "        self.conv8 = SpectralParam(288, 10, kernel_size=1, padding=0)\n",
    "        \n",
    "        self.avg = nn.AvgPool2d(4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = self.pool5(F.relu(self.conv5(x)))\n",
    "        x = self.pool6(F.relu(self.conv6(x)))\n",
    "        x = self.conv8(F.relu(self.conv7(x)))\n",
    "        \n",
    "        return torch.squeeze(self.avg(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  train loss: 57.371  train acc: 0.302  val acc: 0.400  best val acc: 0.400\n",
      "epoch: 2  train loss: 1.438  train acc: 0.484  val acc: 0.520  best val acc: 0.520\n",
      "epoch: 3  train loss: 1.215  train acc: 0.567  val acc: 0.615  best val acc: 0.615\n",
      "epoch: 4  train loss: 1.057  train acc: 0.626  val acc: 0.624  best val acc: 0.624\n",
      "epoch: 5  train loss: 0.945  train acc: 0.667  val acc: 0.643  best val acc: 0.643\n",
      "epoch: 6  train loss: 0.844  train acc: 0.705  val acc: 0.664  best val acc: 0.664\n",
      "epoch: 7  train loss: 0.763  train acc: 0.734  val acc: 0.689  best val acc: 0.689\n",
      "epoch: 8  train loss: 0.712  train acc: 0.749  val acc: 0.695  best val acc: 0.695\n",
      "epoch: 9  train loss: 0.657  train acc: 0.770  val acc: 0.721  best val acc: 0.721\n",
      "epoch: 10  train loss: 0.604  train acc: 0.789  val acc: 0.730  best val acc: 0.730\n",
      "epoch: 11  train loss: 0.564  train acc: 0.803  val acc: 0.715  best val acc: 0.730\n",
      "epoch: 12  train loss: 0.533  train acc: 0.812  val acc: 0.693  best val acc: 0.730\n",
      "epoch: 13  train loss: 0.508  train acc: 0.820  val acc: 0.727  best val acc: 0.730\n",
      "epoch: 14  train loss: 0.477  train acc: 0.832  val acc: 0.715  best val acc: 0.730\n",
      "epoch: 15  train loss: 0.457  train acc: 0.840  val acc: 0.713  best val acc: 0.730\n",
      "epoch: 16  train loss: 0.425  train acc: 0.848  val acc: 0.733  best val acc: 0.733\n",
      "epoch: 17  train loss: 0.429  train acc: 0.847  val acc: 0.678  best val acc: 0.733\n",
      "epoch: 18  train loss: 0.405  train acc: 0.856  val acc: 0.721  best val acc: 0.733\n",
      "epoch: 19  train loss: 0.413  train acc: 0.852  val acc: 0.736  best val acc: 0.736\n",
      "epoch: 20  train loss: 0.385  train acc: 0.862  val acc: 0.723  best val acc: 0.736\n",
      "epoch: 21  train loss: 0.285  train acc: 0.898  val acc: 0.748  best val acc: 0.748\n",
      "epoch: 22  train loss: 0.271  train acc: 0.904  val acc: 0.730  best val acc: 0.748\n",
      "epoch: 23  train loss: 0.291  train acc: 0.895  val acc: 0.739  best val acc: 0.748\n",
      "epoch: 24  train loss: 0.267  train acc: 0.905  val acc: 0.731  best val acc: 0.748\n",
      "epoch: 25  train loss: 0.286  train acc: 0.897  val acc: 0.723  best val acc: 0.748\n",
      "epoch: 26  train loss: 0.284  train acc: 0.898  val acc: 0.745  best val acc: 0.748\n",
      "epoch: 27  train loss: 0.271  train acc: 0.903  val acc: 0.748  best val acc: 0.748\n",
      "epoch: 28  train loss: 0.272  train acc: 0.903  val acc: 0.744  best val acc: 0.748\n",
      "epoch: 29  train loss: 0.272  train acc: 0.903  val acc: 0.743  best val acc: 0.748\n",
      "epoch: 30  train loss: 0.261  train acc: 0.906  val acc: 0.732  best val acc: 0.748\n",
      "epoch: 31  train loss: 0.269  train acc: 0.905  val acc: 0.716  best val acc: 0.748\n",
      "epoch: 32  train loss: 0.249  train acc: 0.911  val acc: 0.748  best val acc: 0.748\n",
      "epoch: 33  train loss: 0.248  train acc: 0.911  val acc: 0.732  best val acc: 0.748\n",
      "epoch: 34  train loss: 0.240  train acc: 0.913  val acc: 0.742  best val acc: 0.748\n",
      "epoch: 35  train loss: 0.235  train acc: 0.917  val acc: 0.753  best val acc: 0.753\n",
      "epoch: 36  train loss: 0.240  train acc: 0.913  val acc: 0.744  best val acc: 0.753\n",
      "epoch: 37  train loss: 0.233  train acc: 0.917  val acc: 0.734  best val acc: 0.753\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4588b43b8b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Assign/Spectral/modules/image_generator.py\u001b[0m in \u001b[0;36mnext_batch_gen\u001b[0;34m(self, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "total_epoch = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = Net(kernel_size).cuda()\n",
    "    \n",
    "    best_val = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)\n",
    "    \n",
    "    img_gen = ImageGenerator(xtrain[:-4096], ytrain[:-4096])\n",
    "    val_gen = ImageGenerator(xtrain[-4096:], ytrain[-4096:])\n",
    "    \n",
    "    generator = img_gen.next_batch_gen(batch_size)\n",
    "    val_generator = val_gen.next_batch_gen(batch_size)\n",
    "    \n",
    "    iters = int((xtrain.shape[0] - 4096) / batch_size)\n",
    "    val_iters = int(4096 / batch_size)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        loss_iter = []\n",
    "        acc_iter = []\n",
    "        for itr in range(iters):\n",
    "            \n",
    "            X_batch, y_batch = next(generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss_iter.append(loss.data.cpu().numpy()[0])\n",
    "            acc_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        train_loss = np.mean(loss_iter)\n",
    "        train_acc = np.sum(acc_iter) / (xtrain.shape[0] - 4096)\n",
    "        \n",
    "        # validation\n",
    "        val_iter = []\n",
    "        for itr in range(val_iters):\n",
    "            X_batch, y_batch = next(val_generator)\n",
    "            inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "            labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "            outputs = net.forward(inputs)\n",
    "            \n",
    "            _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "            val_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "        val_acc = np.sum(val_iter) / 4096\n",
    "        \n",
    "        if best_val < val_acc:\n",
    "            best_val = val_acc\n",
    "            torch.save(net.state_dict(), 'checkpoint.pth.tar')\n",
    "        \n",
    "        print('epoch: %d  train loss: %.3f  train acc: %.3f  val acc: %.3f  best val acc: %.3f' % \n",
    "              (epoch + 1, train_loss, train_acc, val_acc, best_val))\n",
    "    \n",
    "    # test the network\n",
    "    testnet = Net(kernel_size).cuda()\n",
    "    testnet.load_state_dict(torch.load('checkpoint.pth.tar'))\n",
    "    \n",
    "    test_gen = ImageGenerator(xtest, ytest)\n",
    "    generator = test_gen.next_batch_gen(batch_size)\n",
    "    iters = int(xtest.shape[0] / batch_size)\n",
    "    test_iter = []\n",
    "    for itr in range(iters):\n",
    "        X_batch, y_batch = next(val_generator)\n",
    "        inputs = Variable(torch.Tensor(X_batch).cuda())\n",
    "        labels = Variable(torch.LongTensor(y_batch).cuda())\n",
    "        outputs = testnet.forward(inputs)\n",
    "            \n",
    "        _, predict = torch.max(outputs.data, 1)        \n",
    "\n",
    "        test_iter.append(predict.eq(labels.data).cpu().sum())\n",
    "        \n",
    "    test_acc = np.sum(test_iter) / xtest.shape[0]\n",
    "        \n",
    "    print('test acc: %.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's too slow so I didn't continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
